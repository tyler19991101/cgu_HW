{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Adding white-noise channels or all-zeros channels to MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "train_images_with_noise_channels = np.concatenate(\n",
    "    [train_images, np.random.random((len(train_images), 784))], axis=1)\n",
    "\n",
    "train_images_with_zeros_channels = np.concatenate(\n",
    "    [train_images, np.zeros((len(train_images), 784))], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training the same model on MNIST data with noise channels or all-zero channels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 39s 821us/step - loss: 0.6298 - acc: 0.8112 - val_loss: 0.3014 - val_acc: 0.9114\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 42s 872us/step - loss: 0.2468 - acc: 0.9242 - val_loss: 0.1817 - val_acc: 0.9457\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 48s 990us/step - loss: 0.1662 - acc: 0.9493 - val_loss: 0.1852 - val_acc: 0.9436\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 46s 956us/step - loss: 0.1172 - acc: 0.9639 - val_loss: 0.1239 - val_acc: 0.9617\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 46s 949us/step - loss: 0.0854 - acc: 0.9734 - val_loss: 0.1432 - val_acc: 0.9572\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 45s 940us/step - loss: 0.0661 - acc: 0.9788 - val_loss: 0.1226 - val_acc: 0.9636\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 48s 1ms/step - loss: 0.0478 - acc: 0.9849 - val_loss: 0.1209 - val_acc: 0.9677\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 46s 955us/step - loss: 0.0366 - acc: 0.9883 - val_loss: 0.1253 - val_acc: 0.9664\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 47s 970us/step - loss: 0.0277 - acc: 0.9910 - val_loss: 0.1273 - val_acc: 0.9673\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 45s 947us/step - loss: 0.0213 - acc: 0.9931 - val_loss: 0.1501 - val_acc: 0.9636\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "18560/48000 [==========>...................] - ETA: 26s - loss: 0.4341 - acc: 0.8720"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(512, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"sparse_categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "history_noise = model.fit(\n",
    "    train_images_with_noise_channels, train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)\n",
    "\n",
    "model = get_model()\n",
    "history_zeros = model.fit(\n",
    "    train_images_with_zeros_channels, train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting a validation accuracy comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_acc_noise = history_noise.history[\"val_acc\"]\n",
    "val_acc_zeros = history_zeros.history[\"val_acc\"]\n",
    "epochs = range(1, 11)\n",
    "plt.plot(epochs, val_acc_noise, \"b-\",\n",
    "         label=\"Validation accuracy with noise channels\")\n",
    "plt.plot(epochs, val_acc_zeros, \"b--\",\n",
    "         label=\"Validation accuracy with zeros channels\")\n",
    "plt.title(\"Effect of noise channels on validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The nature of generalization in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Fitting a MNIST model with randomly shuffled labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "random_train_labels = train_labels[:]\n",
    "np.random.shuffle(random_train_labels)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "#print(dir(model))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, random_train_labels,\n",
    "          epochs=100,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Improving model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Tuning key gradient descent parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training a MNIST model with an incorrectly high learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 1s 16us/sample - loss: 14.4310 - acc: 0.1026 - val_loss: 14.4499 - val_acc: 0.1035\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 14.4764 - acc: 0.1019 - val_loss: 14.4499 - val_acc: 0.1035\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 14.4764 - acc: 0.1019 - val_loss: 14.4499 - val_acc: 0.1035\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 14.4764 - acc: 0.1019 - val_loss: 14.4499 - val_acc: 0.1035\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 14.4764 - acc: 0.1019 - val_loss: 14.4499 - val_acc: 0.1035\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 14.4764 - acc: 0.1019 - val_loss: 14.4499 - val_acc: 0.1035\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 14.4764 - acc: 0.1019 - val_loss: 14.4499 - val_acc: 0.1035\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 14.4764 - acc: 0.1019 - val_loss: 14.4499 - val_acc: 0.1035\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 14.4764 - acc: 0.1019 - val_loss: 14.4499 - val_acc: 0.1035\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 14.4764 - acc: 0.1019 - val_loss: 14.4499 - val_acc: 0.1035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2588ca3f988>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_images, train_labels), _ = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1.),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**The same model with a more appropriate learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.4411 - acc: 0.9067 - val_loss: 0.1829 - val_acc: 0.9506\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1463 - acc: 0.9639 - val_loss: 0.1659 - val_acc: 0.9643\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1172 - acc: 0.9729 - val_loss: 0.1542 - val_acc: 0.9700\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.1020 - acc: 0.9780 - val_loss: 0.2199 - val_acc: 0.9625\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0909 - acc: 0.9812 - val_loss: 0.2030 - val_acc: 0.9677\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0860 - acc: 0.9837 - val_loss: 0.2244 - val_acc: 0.9693\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0867 - acc: 0.9850 - val_loss: 0.2260 - val_acc: 0.9735\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0776 - acc: 0.9868 - val_loss: 0.2200 - val_acc: 0.9743\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0797 - acc: 0.9873 - val_loss: 0.2216 - val_acc: 0.9734\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.0697 - acc: 0.9890 - val_loss: 0.2156 - val_acc: 0.9744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2588cdded88>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-2),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Leveraging better architecture priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Increasing model capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**A simple logistic regression on MNIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.6735 - acc: 0.8324 - val_loss: 0.3594 - val_acc: 0.9047\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3514 - acc: 0.9030 - val_loss: 0.3083 - val_acc: 0.9144\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.3154 - acc: 0.9125 - val_loss: 0.2935 - val_acc: 0.9181\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2995 - acc: 0.9164 - val_loss: 0.2827 - val_acc: 0.9208\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2898 - acc: 0.9185 - val_loss: 0.2766 - val_acc: 0.9241\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2831 - acc: 0.9209 - val_loss: 0.2729 - val_acc: 0.9253\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2780 - acc: 0.9224 - val_loss: 0.2710 - val_acc: 0.9257\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2742 - acc: 0.9234 - val_loss: 0.2683 - val_acc: 0.9275\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2710 - acc: 0.9235 - val_loss: 0.2666 - val_acc: 0.9271\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2687 - acc: 0.9256 - val_loss: 0.2653 - val_acc: 0.9281\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2663 - acc: 0.9261 - val_loss: 0.2632 - val_acc: 0.9296\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2645 - acc: 0.9266 - val_loss: 0.2644 - val_acc: 0.9297\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2629 - acc: 0.9280 - val_loss: 0.2622 - val_acc: 0.9304\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.2614 - acc: 0.9280 - val_loss: 0.2614 - val_acc: 0.9306\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2601 - acc: 0.9281 - val_loss: 0.2630 - val_acc: 0.9308\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2588 - acc: 0.9287 - val_loss: 0.2626 - val_acc: 0.9303\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2578 - acc: 0.9289 - val_loss: 0.2644 - val_acc: 0.9295\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2570 - acc: 0.9293 - val_loss: 0.2620 - val_acc: 0.9308\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2561 - acc: 0.9298 - val_loss: 0.2610 - val_acc: 0.9306\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 12us/sample - loss: 0.2549 - acc: 0.9303 - val_loss: 0.2620 - val_acc: 0.9305\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([layers.Dense(10, activation=\"softmax\")])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_small_model = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2588e0cc688>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "val_loss = history_small_model.history[\"val_loss\"]\n",
    "epochs = range(1, 21)\n",
    "plt.plot(epochs, val_loss, \"b--\",\n",
    "         label=\"Validation loss\")\n",
    "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3511 - acc: 0.8995 - val_loss: 0.1829 - val_acc: 0.9485\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 15us/sample - loss: 0.1566 - acc: 0.9532 - val_loss: 0.1330 - val_acc: 0.9611\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.1098 - acc: 0.9676 - val_loss: 0.1129 - val_acc: 0.9681\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0845 - acc: 0.9745 - val_loss: 0.1071 - val_acc: 0.9688\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0674 - acc: 0.9797 - val_loss: 0.1034 - val_acc: 0.9711\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0561 - acc: 0.9827 - val_loss: 0.0915 - val_acc: 0.9738\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0463 - acc: 0.9854 - val_loss: 0.0927 - val_acc: 0.9750\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0388 - acc: 0.9885 - val_loss: 0.0983 - val_acc: 0.9733\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0322 - acc: 0.9905 - val_loss: 0.1014 - val_acc: 0.9721\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0275 - acc: 0.9915 - val_loss: 0.1140 - val_acc: 0.9712\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0223 - acc: 0.9931 - val_loss: 0.0958 - val_acc: 0.9772\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0191 - acc: 0.9939 - val_loss: 0.1081 - val_acc: 0.9726\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0156 - acc: 0.9952 - val_loss: 0.1154 - val_acc: 0.9739\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 1s 13us/sample - loss: 0.0132 - acc: 0.9957 - val_loss: 0.1208 - val_acc: 0.9740\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0119 - acc: 0.9961 - val_loss: 0.1236 - val_acc: 0.9728\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0096 - acc: 0.9970 - val_loss: 0.1183 - val_acc: 0.9767\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0087 - acc: 0.9973 - val_loss: 0.1276 - val_acc: 0.9762\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0079 - acc: 0.9974 - val_loss: 0.1208 - val_acc: 0.9772\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0064 - acc: 0.9980 - val_loss: 0.1287 - val_acc: 0.9773\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 14us/sample - loss: 0.0061 - acc: 0.9981 - val_loss: 0.1447 - val_acc: 0.9733\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(96, activation=\"relu\"),\n",
    "    layers.Dense(96, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_large_model = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Improving generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Dataset curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Using early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Regularizing your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Reducing the network's size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Original model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:128: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\user\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 120us/sample - loss: 0.5075 - acc: 0.7809 - val_loss: 0.3902 - val_acc: 0.8521\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 95us/sample - loss: 0.2979 - acc: 0.9038 - val_loss: 0.3091 - val_acc: 0.8806\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.2188 - acc: 0.9297 - val_loss: 0.2816 - val_acc: 0.8867\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.1711 - acc: 0.9443 - val_loss: 0.2984 - val_acc: 0.8804\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.1372 - acc: 0.9561 - val_loss: 0.2952 - val_acc: 0.8861\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.1123 - acc: 0.9647 - val_loss: 0.3469 - val_acc: 0.8718\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.0926 - acc: 0.9735 - val_loss: 0.3329 - val_acc: 0.8804\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 99us/sample - loss: 0.0782 - acc: 0.9775 - val_loss: 0.3397 - val_acc: 0.8836\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.0637 - acc: 0.9826 - val_loss: 0.3647 - val_acc: 0.8803\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.0513 - acc: 0.9864 - val_loss: 0.3938 - val_acc: 0.8773\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 95us/sample - loss: 0.0431 - acc: 0.9892 - val_loss: 0.4272 - val_acc: 0.8734\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 82us/sample - loss: 0.0359 - acc: 0.9920 - val_loss: 0.4577 - val_acc: 0.8747\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 95us/sample - loss: 0.0262 - acc: 0.9945 - val_loss: 0.4860 - val_acc: 0.8728\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.0220 - acc: 0.9950 - val_loss: 0.5200 - val_acc: 0.8712\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 98us/sample - loss: 0.0159 - acc: 0.9977 - val_loss: 0.5599 - val_acc: 0.8692\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.0133 - acc: 0.9982 - val_loss: 0.5820 - val_acc: 0.8681\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 0.0142 - acc: 0.9969 - val_loss: 0.6123 - val_acc: 0.8675\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 99us/sample - loss: 0.0061 - acc: 0.9996 - val_loss: 0.6451 - val_acc: 0.8662\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.0105 - acc: 0.9977 - val_loss: 0.6780 - val_acc: 0.8652\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.0036 - acc: 0.9996 - val_loss: 0.7054 - val_acc: 0.8633\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data, train_labels), _ = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "train_data = vectorize_sequences(train_data)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_original = model.fit(train_data, train_labels,\n",
    "                             epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Version of the model with lower capacity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 108us/sample - loss: 0.6223 - acc: 0.6323 - val_loss: 0.5756 - val_acc: 0.6643\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.5297 - acc: 0.7720 - val_loss: 0.5346 - val_acc: 0.7389\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 0.4794 - acc: 0.8505 - val_loss: 0.4989 - val_acc: 0.8408\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.4436 - acc: 0.8873 - val_loss: 0.4901 - val_acc: 0.8258\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 96us/sample - loss: 0.4159 - acc: 0.9110 - val_loss: 0.4699 - val_acc: 0.8654\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 83us/sample - loss: 0.3926 - acc: 0.9279 - val_loss: 0.4613 - val_acc: 0.8702\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 0.3714 - acc: 0.9424 - val_loss: 0.4692 - val_acc: 0.8528\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.3510 - acc: 0.9519 - val_loss: 0.4509 - val_acc: 0.8737\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 0.3330 - acc: 0.9619 - val_loss: 0.4464 - val_acc: 0.8722\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.3167 - acc: 0.9687 - val_loss: 0.4588 - val_acc: 0.8652\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.3008 - acc: 0.9745 - val_loss: 0.4561 - val_acc: 0.8668\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 90us/sample - loss: 0.2849 - acc: 0.9785 - val_loss: 0.4746 - val_acc: 0.8602\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.2708 - acc: 0.9819 - val_loss: 0.4647 - val_acc: 0.8653\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.2572 - acc: 0.9850 - val_loss: 0.4850 - val_acc: 0.8615\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 93us/sample - loss: 0.2435 - acc: 0.9869 - val_loss: 0.4867 - val_acc: 0.8617\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 81us/sample - loss: 0.2309 - acc: 0.9881 - val_loss: 0.4884 - val_acc: 0.8624\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.2191 - acc: 0.9892 - val_loss: 0.4843 - val_acc: 0.8659\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.2069 - acc: 0.9903 - val_loss: 0.5112 - val_acc: 0.8615\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 94us/sample - loss: 0.1961 - acc: 0.9911 - val_loss: 0.4781 - val_acc: 0.8680\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 0.1853 - acc: 0.9919 - val_loss: 0.6163 - val_acc: 0.8453\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(4, activation=\"relu\"),\n",
    "    layers.Dense(4, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_smaller_model = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Version of the model with higher capacity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 110us/sample - loss: 0.5534 - acc: 0.7469 - val_loss: 0.3141 - val_acc: 0.8743\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 82us/sample - loss: 0.2494 - acc: 0.9049 - val_loss: 0.3422 - val_acc: 0.8500\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 81us/sample - loss: 0.1507 - acc: 0.9425 - val_loss: 0.2984 - val_acc: 0.8922\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.1047 - acc: 0.9651 - val_loss: 0.3438 - val_acc: 0.8706\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.0811 - acc: 0.9785 - val_loss: 0.3121 - val_acc: 0.8867\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.0069 - acc: 0.9996 - val_loss: 0.4853 - val_acc: 0.8865\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 100us/sample - loss: 8.3856e-04 - acc: 0.9999 - val_loss: 0.5879 - val_acc: 0.8867\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 80us/sample - loss: 9.3391e-05 - acc: 1.0000 - val_loss: 0.6796 - val_acc: 0.8864\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 83us/sample - loss: 1.6351e-05 - acc: 1.0000 - val_loss: 0.7517 - val_acc: 0.8848\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 83us/sample - loss: 3.6843e-06 - acc: 1.0000 - val_loss: 0.8201 - val_acc: 0.8849\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 1.0815e-06 - acc: 1.0000 - val_loss: 0.8580 - val_acc: 0.8861\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 3.8547e-07 - acc: 1.0000 - val_loss: 0.9031 - val_acc: 0.8846\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 97us/sample - loss: 1.9707e-07 - acc: 1.0000 - val_loss: 0.9320 - val_acc: 0.8861\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 80us/sample - loss: 1.4160e-07 - acc: 1.0000 - val_loss: 0.9489 - val_acc: 0.8855\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 1.2485e-07 - acc: 1.0000 - val_loss: 0.9643 - val_acc: 0.8864\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 84us/sample - loss: 1.1878e-07 - acc: 1.0000 - val_loss: 0.9738 - val_acc: 0.8863\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 1.1556e-07 - acc: 1.0000 - val_loss: 0.9816 - val_acc: 0.8857\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 1.1390e-07 - acc: 1.0000 - val_loss: 0.9861 - val_acc: 0.8862\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 1.1287e-07 - acc: 1.0000 - val_loss: 0.9895 - val_acc: 0.8864\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 1.1207e-07 - acc: 1.0000 - val_loss: 0.9942 - val_acc: 0.8859\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_larger_model = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Adding weight regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Adding L2 weight regularization to the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 1s 97us/sample - loss: 0.6037 - acc: 0.7865 - val_loss: 0.4931 - val_acc: 0.8546\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 93us/sample - loss: 0.4135 - acc: 0.8923 - val_loss: 0.4217 - val_acc: 0.8668\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.3437 - acc: 0.9114 - val_loss: 0.3731 - val_acc: 0.8859\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 100us/sample - loss: 0.3082 - acc: 0.9235 - val_loss: 0.3655 - val_acc: 0.8860\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.2861 - acc: 0.9301 - val_loss: 0.3598 - val_acc: 0.8861\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 88us/sample - loss: 0.2698 - acc: 0.9345 - val_loss: 0.3847 - val_acc: 0.8742\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 85us/sample - loss: 0.2592 - acc: 0.9403 - val_loss: 0.3590 - val_acc: 0.8863\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 91us/sample - loss: 0.2530 - acc: 0.9403 - val_loss: 0.3714 - val_acc: 0.8815\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.2414 - acc: 0.9464 - val_loss: 0.3697 - val_acc: 0.8812\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 86us/sample - loss: 0.2379 - acc: 0.9469 - val_loss: 0.3690 - val_acc: 0.8823\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 2s 100us/sample - loss: 0.2316 - acc: 0.9491 - val_loss: 0.3814 - val_acc: 0.8779\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 0.2255 - acc: 0.9509 - val_loss: 0.3896 - val_acc: 0.8751\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 87us/sample - loss: 0.2252 - acc: 0.9514 - val_loss: 0.3796 - val_acc: 0.8811\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 93us/sample - loss: 0.2227 - acc: 0.9520 - val_loss: 0.3836 - val_acc: 0.8787\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 90us/sample - loss: 0.2130 - acc: 0.9564 - val_loss: 0.3863 - val_acc: 0.8779\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 0.2106 - acc: 0.9571 - val_loss: 0.4054 - val_acc: 0.8729\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 0.2187 - acc: 0.9513 - val_loss: 0.4029 - val_acc: 0.8747\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 90us/sample - loss: 0.2007 - acc: 0.9627 - val_loss: 0.4339 - val_acc: 0.8700\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 89us/sample - loss: 0.2022 - acc: 0.9596 - val_loss: 0.4039 - val_acc: 0.8744\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 95us/sample - loss: 0.2087 - acc: 0.9553 - val_loss: 0.4051 - val_acc: 0.8749\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002),\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(16,\n",
    "                 kernel_regularizer=regularizers.l2(0.002),\n",
    "                 activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_l2_reg = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Different weight regularizers available in Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.regularizers.L1L2 at 0x2588ed9cb88>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "regularizers.l1(0.001)\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Adding dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Adding dropout to the IMDB model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 2s 108us/sample - loss: 0.6192 - acc: 0.6461 - val_loss: 0.4983 - val_acc: 0.8552\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 99us/sample - loss: 0.4942 - acc: 0.7698 - val_loss: 0.3929 - val_acc: 0.8723\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 96us/sample - loss: 0.4179 - acc: 0.8211 - val_loss: 0.3241 - val_acc: 0.8852\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 97us/sample - loss: 0.3548 - acc: 0.8626 - val_loss: 0.2999 - val_acc: 0.8799\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 96us/sample - loss: 0.3082 - acc: 0.8901 - val_loss: 0.2790 - val_acc: 0.8923\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 100us/sample - loss: 0.2668 - acc: 0.9099 - val_loss: 0.2972 - val_acc: 0.8847\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 96us/sample - loss: 0.2319 - acc: 0.9247 - val_loss: 0.2821 - val_acc: 0.8884\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 98us/sample - loss: 0.2038 - acc: 0.9359 - val_loss: 0.2979 - val_acc: 0.8817\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 96us/sample - loss: 0.1810 - acc: 0.9431 - val_loss: 0.3102 - val_acc: 0.8902\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 96us/sample - loss: 0.1599 - acc: 0.9492 - val_loss: 0.3387 - val_acc: 0.8886\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 96us/sample - loss: 0.1450 - acc: 0.9556 - val_loss: 0.3599 - val_acc: 0.8888\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 97us/sample - loss: 0.1257 - acc: 0.9611 - val_loss: 0.3830 - val_acc: 0.8772\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 98us/sample - loss: 0.1180 - acc: 0.9631 - val_loss: 0.3979 - val_acc: 0.8827\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 97us/sample - loss: 0.1100 - acc: 0.9651 - val_loss: 0.4321 - val_acc: 0.8867\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 97us/sample - loss: 0.0983 - acc: 0.9689 - val_loss: 0.5205 - val_acc: 0.8817\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 96us/sample - loss: 0.0895 - acc: 0.9735 - val_loss: 0.4670 - val_acc: 0.8848\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 98us/sample - loss: 0.0890 - acc: 0.9712 - val_loss: 0.5044 - val_acc: 0.8827\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 96us/sample - loss: 0.0818 - acc: 0.9761 - val_loss: 0.5591 - val_acc: 0.8832\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 99us/sample - loss: 0.0740 - acc: 0.9769 - val_loss: 0.5833 - val_acc: 0.8832\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 96us/sample - loss: 0.0744 - acc: 0.9776 - val_loss: 0.5795 - val_acc: 0.8824\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history_dropout = model.fit(\n",
    "    train_data, train_labels,\n",
    "    epochs=20, batch_size=512, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "train_data = vectorize_sequences(train_data)\n",
    "test_data = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 3s 124us/sample - loss: 0.1200 - mean_absolute_error: 0.1832\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(train_data, train_labels,\n",
    "          epochs=130, batch_size=512, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18316545"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mae_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6595273], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = model.predict(test_data)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter05_fundamentals-of-ml.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
